{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acheng416/Documents/CS229_Project/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from fevd_vqvae.models import VQModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from fevd_vqvae.models.model_modules import Encoder\n",
    "from fevd_vqvae.models.vector_quantizer import VectorQuantizer\n",
    "from fevd_vqvae.models.utils import instantiate_from_config\n",
    "from fevd_vqvae.models.loss import VQLoss\n",
    "from omegaconf import OmegaConf\n",
    "from fevd_vqvae.utils import setup_dataloader\n",
    "from fevd_vqvae.models.model_modules import Normalize\n",
    "cfg_dict = OmegaConf.load(\"configs/baseline.yaml\")\n",
    "model_cfg_dict = cfg_dict['model']\n",
    "train_cfg_dict = cfg_dict['setup']\n",
    "model_cfg_dict\n",
    "device = torch.device('cuda')\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'key': '2d', 'embed_dim': 192, 'n_embed': 1024, 'ddconfig': {'double_z': False, 'z_channels': 256, 'resolution': 64, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 1, 2, 2, 4], 'num_res_blocks': 2, 'attn_resolutions': [16], 'dropout': 0.0}, 'lossconfig': {'codebook_weight': 1.0, 'pixelloss_weight': 1.0, 'perceptual_weight_2d': 1.0, 'fvd_mu_weight': 0.0, 'fvd_cov_weight': 0.0}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cfg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VQModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 key,\n",
    "                 ddconfig,\n",
    "                 lossconfig,\n",
    "                 n_embed,\n",
    "                 embed_dim,\n",
    "                 ckpt_sd=None):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(**ddconfig)\n",
    "        self.loss = VQLoss(**lossconfig)\n",
    "        self.quantize = VectorQuantizer(n_embed, embed_dim, beta=0.25)\n",
    "        self.quant_conv = nn.Conv2d(ddconfig[\"z_channels\"], embed_dim, 1)\n",
    "\n",
    "        if key == '2d':\n",
    "            #self.decoder = Decoder2D(**ddconfig)\n",
    "            self.post_quant_conv = nn.Conv3d(embed_dim, ddconfig[\"z_channels\"], 1)\n",
    "\n",
    "    def encode(self, x):\n",
    "\n",
    "        input_is_videos = len(x.shape) == 5\n",
    "        if input_is_videos:\n",
    "            B, T, C, H, W = x.shape  # the input is a batch of videos (B, T, C, H, W)\n",
    "            x = x.reshape(B * T, C, H, W)\n",
    "\n",
    "        h = self.encoder(x)\n",
    "        h = self.quant_conv(h)\n",
    "        quant, emb_loss, info = self.quantize(h)\n",
    "\n",
    "        if input_is_videos:\n",
    "            _, C, H, W = quant.shape\n",
    "            quant = quant.reshape(B, T, C, H, W)\n",
    "        return quant, emb_loss, info\n",
    "\n",
    "    def forward(self, real_videos):\n",
    "        quant, codebook_loss, _ = self.encode(real_videos)\n",
    "        print(\"Quant Shape: \", quant.shape)\n",
    "        quant = quant.permute((0, 2, 1, 3, 4))\n",
    "        return self.post_quant_conv(quant)\n",
    "        #rec_videos = self.decode(quant)\n",
    "        #return rec_videos, codebook_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQModel(**model_cfg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = setup_dataloader(root_dir_path=\"data/dataset/preprocessed_data/\", **train_cfg_dict['train_dataloader'])\n",
    "eval_dataloaders = setup_dataloader(root_dir_path=\"data/dataset/preprocessed_data/\", **train_cfg_dict['eval_dataloader'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinearity(x):\n",
    "    # swish\n",
    "    return x*torch.sigmoid(x)\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, in_channels, with_conv):\n",
    "        super().__init__()\n",
    "        self.with_conv = with_conv\n",
    "        if self.with_conv:\n",
    "            self.conv = torch.nn.Conv3d(in_channels,\n",
    "                                        in_channels,\n",
    "                                        kernel_size=3,\n",
    "                                        stride=1,\n",
    "                                        padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n",
    "        if self.with_conv:\n",
    "            x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class AttnBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.norm = Normalize(in_channels)\n",
    "        self.q = torch.nn.Conv3d(in_channels,\n",
    "                                 in_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "        self.k = torch.nn.Conv3d(in_channels,\n",
    "                                 in_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "        self.v = torch.nn.Conv3d(in_channels,\n",
    "                                 in_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "        self.proj_out = torch.nn.Conv3d(in_channels,\n",
    "                                        in_channels,\n",
    "                                        kernel_size=1,\n",
    "                                        stride=1,\n",
    "                                        padding=0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_ = x\n",
    "        h_ = self.norm(h_)\n",
    "        q = self.q(h_)\n",
    "        k = self.k(h_)\n",
    "        v = self.v(h_)\n",
    "\n",
    "        # Compute spatiotemporal attention\n",
    "        b,c,t, h,w = q.shape\n",
    "        print(\"Q Shape: \", q.shape)\n",
    "        q = q.reshape(b,c, t*h*w)\n",
    "        q = q.permute(0,2,1)   # b,t*hw,c\n",
    "        k = k.reshape(b,c,t*h*w) # b,c,t*hw\n",
    "        w_ = torch.bmm(q,k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n",
    "        w_ = w_ * (int(c)**(-0.5))\n",
    "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
    "\n",
    "        # attend to values\n",
    "        v = v.reshape(b,c,t*h*w)\n",
    "        w_ = w_.permute(0,2,1)   # b,hw,hw (first hw of k, second of q)\n",
    "        h_ = torch.bmm(v,w_)     # b, c,t*hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n",
    "        h_ = h_.reshape(b,c,t,h,w)\n",
    "\n",
    "        h_ = self.proj_out(h_)\n",
    "\n",
    "        return x+h_\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n",
    "                 dropout, temb_channels=512):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        out_channels = in_channels if out_channels is None else out_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.use_conv_shortcut = conv_shortcut\n",
    "\n",
    "        self.norm1 = Normalize(in_channels)\n",
    "        self.conv1 = torch.nn.Conv3d(in_channels,\n",
    "                                     out_channels,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=1,\n",
    "                                     padding=1)\n",
    "        if temb_channels > 0:\n",
    "            self.temb_proj = torch.nn.Linear(temb_channels,\n",
    "                                             out_channels)\n",
    "        self.norm2 = Normalize(out_channels)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.conv2 = torch.nn.Conv3d(out_channels,\n",
    "                                     out_channels,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=1,\n",
    "                                     padding=1)\n",
    "        if self.in_channels != self.out_channels:\n",
    "            if self.use_conv_shortcut:\n",
    "                self.conv_shortcut = torch.nn.Conv3d(in_channels,\n",
    "                                                     out_channels,\n",
    "                                                     kernel_size=3,\n",
    "                                                     stride=1,\n",
    "                                                     padding=1)\n",
    "            else:\n",
    "                self.nin_shortcut = torch.nn.Conv3d(in_channels,\n",
    "                                                    out_channels,\n",
    "                                                    kernel_size=1,\n",
    "                                                    stride=1,\n",
    "                                                    padding=0)\n",
    "\n",
    "    def forward(self, x, temb):\n",
    "        h = x\n",
    "        h = self.norm1(h)\n",
    "        h = nonlinearity(h)\n",
    "        h = self.conv1(h)\n",
    "\n",
    "        if temb is not None:\n",
    "            h = h + self.temb_proj(nonlinearity(temb))[:,:,None,None]\n",
    "\n",
    "        h = self.norm2(h)\n",
    "        h = nonlinearity(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv2(h)\n",
    "\n",
    "        if self.in_channels != self.out_channels:\n",
    "            if self.use_conv_shortcut:\n",
    "                x = self.conv_shortcut(x)\n",
    "            else:\n",
    "                x = self.nin_shortcut(x)\n",
    "\n",
    "        return x+h\n",
    "\n",
    "class Decoder3D(torch.nn.Module):\n",
    "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
    "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
    "                 resolution, z_channels, give_pre_end=False, **ignorekwargs):\n",
    "        super().__init__()\n",
    "        self.ch = ch\n",
    "        self.temb_ch = 0\n",
    "        self.num_resolutions = len(ch_mult)\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.resolution = resolution\n",
    "        self.in_channels = in_channels\n",
    "        self.give_pre_end = give_pre_end\n",
    "\n",
    "        # compute in_ch_mult, block_in and curr_res at lowest res\n",
    "        in_ch_mult = (1,)+tuple(ch_mult)\n",
    "        block_in = ch*ch_mult[self.num_resolutions-1]\n",
    "        curr_res = resolution // 2**(self.num_resolutions-1)\n",
    "        self.z_shape = (1,z_channels,curr_res,curr_res)\n",
    "        print(\"Working with z of shape {} = {} dimensions.\".format(\n",
    "            self.z_shape, np.prod(self.z_shape)))\n",
    "\n",
    "        # z to block_in\n",
    "        self.conv_in = torch.nn.Conv3d(z_channels,\n",
    "                                       block_in,\n",
    "                                       kernel_size=3,\n",
    "                                       stride=1,\n",
    "                                       padding=1)\n",
    "\n",
    "        # middle\n",
    "        self.mid = nn.Module()\n",
    "        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n",
    "                                       out_channels=block_in,\n",
    "                                       temb_channels=self.temb_ch,\n",
    "                                       dropout=dropout)\n",
    "        self.mid.attn_1 = AttnBlock(block_in)\n",
    "        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n",
    "                                       out_channels=block_in,\n",
    "                                       temb_channels=self.temb_ch,\n",
    "                                       dropout=dropout)\n",
    "\n",
    "        # upsampling\n",
    "        self.up = nn.ModuleList()\n",
    "        for i_level in reversed(range(self.num_resolutions)):\n",
    "            block = nn.ModuleList()\n",
    "            attn = nn.ModuleList()\n",
    "            block_out = ch*ch_mult[i_level]\n",
    "            for i_block in range(self.num_res_blocks+1):\n",
    "                block.append(ResnetBlock(in_channels=block_in,\n",
    "                                         out_channels=block_out,\n",
    "                                         temb_channels=self.temb_ch,\n",
    "                                         dropout=dropout))\n",
    "                block_in = block_out\n",
    "                if curr_res in attn_resolutions:\n",
    "                    attn.append(AttnBlock(block_in))\n",
    "            up = nn.Module()\n",
    "            up.block = block\n",
    "            up.attn = attn\n",
    "            if i_level != 0:\n",
    "                up.upsample = Upsample(block_in, resamp_with_conv)\n",
    "                curr_res = curr_res * 2\n",
    "            self.up.insert(0, up) # prepend to get consistent order\n",
    "\n",
    "        # end\n",
    "        self.norm_out = Normalize(block_in)\n",
    "        self.conv_out = torch.nn.Conv2d(block_in,\n",
    "                                        out_ch,\n",
    "                                        kernel_size=3,\n",
    "                                        stride=1,\n",
    "                                        padding=1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        print(\"Input Z: \", z.shape)\n",
    "        self.last_z_shape = z.shape\n",
    "\n",
    "        # timestep embedding\n",
    "        temb = None\n",
    "\n",
    "        # z to block_in\n",
    "        h = self.conv_in(z)\n",
    "        print(\"Conv_In: \", h.shape)\n",
    "\n",
    "        # middle\n",
    "        h = self.mid.block_1(h, temb)\n",
    "        print(\"Block 1: \", h.shape)\n",
    "        h = self.mid.attn_1(h)\n",
    "        print(\"ATTN : \", h.shape)\n",
    "        h = self.mid.block_2(h, temb)\n",
    "        print(\"Block 2: \", h.shape)\n",
    "\n",
    "        # upsampling\n",
    "        for i_level in reversed(range(self.num_resolutions)):\n",
    "            for i_block in range(self.num_res_blocks+1):\n",
    "                h = self.up[i_level].block[i_block](h, temb)\n",
    "                if len(self.up[i_level].attn) > 0:\n",
    "                    h = self.up[i_level].attn[i_block](h)\n",
    "            if i_level != 0:\n",
    "                h = self.up[i_level].upsample(h)\n",
    "\n",
    "        # end\n",
    "        if self.give_pre_end:\n",
    "            return h\n",
    "\n",
    "        h = self.norm_out(h)\n",
    "        h = nonlinearity(h)\n",
    "        h = self.conv_out(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 4, 4) = 4096 dimensions.\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder3D(**model_cfg_dict['ddconfig'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 12, 3, 64, 64])\n",
      "Quant Shape:  torch.Size([10, 12, 192, 4, 4])\n",
      "torch.Size([10, 256, 12, 4, 4])\n",
      "Input Z:  torch.Size([10, 256, 12, 4, 4])\n",
      "Conv_In:  torch.Size([10, 512, 12, 4, 4])\n",
      "Block 1:  torch.Size([10, 512, 12, 4, 4])\n",
      "Q Shape:  torch.Size([10, 512, 12, 4, 4])\n",
      "ATTN :  torch.Size([10, 512, 12, 4, 4])\n",
      "Block 2:  torch.Size([10, 512, 12, 4, 4])\n",
      "Q Shape:  torch.Size([10, 256, 48, 16, 16])\n",
      "Q Shape:  torch.Size([10, 256, 48, 16, 16])\n",
      "Q Shape:  torch.Size([10, 256, 48, 16, 16])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for m, x in enumerate(eval_dataloaders['train']):\n",
    "    print(x.shape)\n",
    "    enc = model(x)\n",
    "    print(enc.shape)\n",
    "    #print(x.shape) # (N, T, C, H, W)\n",
    "    #x = x.permute((0, 2, 1, 3, 4)) #(N, C, T, H, W) for 3D conv\n",
    "    #print(model(x).shape)\n",
    "    break\n",
    "decoded = decoder(enc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c54350b54d11f24740b330755d09daeae7491ad712c26841bc474854156871c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
